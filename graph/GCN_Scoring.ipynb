{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T06:03:36.717836Z",
     "start_time": "2019-07-01T06:03:36.711941Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import torch as th\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-30T13:04:46.500781Z",
     "start_time": "2019-06-30T13:04:46.496724Z"
    }
   },
   "outputs": [],
   "source": [
    "#os.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-30T13:04:47.842256Z",
     "start_time": "2019-06-30T13:04:47.591041Z"
    }
   },
   "outputs": [],
   "source": [
    "feature = pd.read_csv('./node_feature.tsv', sep='\\t', header=0)\n",
    "\n",
    "# feature file contains sample id (apply_no)  and all the feature extracted by expertise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-30T13:04:48.719995Z",
     "start_time": "2019-06-30T13:04:48.577590Z"
    }
   },
   "outputs": [],
   "source": [
    "INF=1e10\n",
    "feature.replace(np.inf, INF,inplace=True)\n",
    "feature.replace(np.nan, 0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-30T13:05:28.810681Z",
     "start_time": "2019-06-30T13:05:28.792653Z"
    }
   },
   "outputs": [],
   "source": [
    "categorical_ft=[]\n",
    "numerical_ft = []\n",
    "for col in feature.columns:\n",
    "    if col in exclude_list:\n",
    "        continue\n",
    "    else:\n",
    "        if feature[col].dtype == np.object:\n",
    "            categorical_ft.append(col)\n",
    "        else:\n",
    "            numerical_ft.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-30T13:05:29.350551Z",
     "start_time": "2019-06-30T13:05:29.344991Z"
    }
   },
   "outputs": [],
   "source": [
    "not_null_norm=ColumnTransformer(\n",
    "    [('numerical', MinMaxScaler(copy=True),numerical_ft)]\n",
    ")\n",
    "null_norm=SimpleImputer(missing_values=np.nan, strategy='constant',fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-30T13:05:29.948886Z",
     "start_time": "2019-06-30T13:05:29.895206Z"
    }
   },
   "outputs": [],
   "source": [
    "norm_feature=not_null_norm.fit_transform(feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T06:03:18.549966Z",
     "start_time": "2019-07-01T06:03:18.544482Z"
    }
   },
   "outputs": [],
   "source": [
    "import dgl\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl.nn.pytorch import GraphConv\n",
    "from scipy.stats import ks_2samp\n",
    "from cytoolz import pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T06:03:19.128430Z",
     "start_time": "2019-07-01T06:03:19.124333Z"
    }
   },
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T06:03:19.637952Z",
     "start_time": "2019-07-01T06:03:19.633984Z"
    }
   },
   "outputs": [],
   "source": [
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T06:18:49.075340Z",
     "start_time": "2019-07-01T06:18:49.055001Z"
    }
   },
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "    def __init__(self,\n",
    "                 feature, \n",
    "                 sample_driver,  # driver with apply_no/apply_date/label; matching label to feature extracted\n",
    "                 node_set,\n",
    "                 edge_set,\n",
    "                 ft_col, # list of feature name used\n",
    "                 transformer):\n",
    "        \"\"\"\n",
    "        number of nodes \n",
    "        nodeid\n",
    "        source node \n",
    "        destination node \n",
    "        \n",
    "        \"\"\"\n",
    "        self.num_nodes=node_dataframe.shape[0]\n",
    "        self.node_id=dict(zip([x[1] for x in node_set], range(self.num_nodes))) # see 2_graph_data_explore\n",
    "        \n",
    "        self.g=dgl.DGLGraph()\n",
    "        self.g.add_nodes(self.num_nodes)\n",
    "        for item in edge_set:\n",
    "            self.g.add_edges(item[1], item[2]) # see 2_graph_data_explore\n",
    "        self.g.ndata['nodeId']=feature[v_col].values\n",
    "        \n",
    "        norm_feature=transformer.fit_transform(feature[ft_col])\n",
    "        \n",
    "        self.g.ndata['feature']=th.FloatTensor(norm_feature)\n",
    "        self.g.ndata['label']=th.LongTensor(feature['label'].values)\n",
    "    def apply_mask(self):\n",
    "        y=self.g.ndata['label']\n",
    "        train_mask= np.zeros(y.shape[0],dtype=np.bool)\n",
    "        test_mask= np.zeros(y.shape[0], dtype= np.bool)\n",
    "        #pdb.set_trace()\n",
    "        y0=np.argwhere(y==0)[:,0]\n",
    "        y1=np.argwhere(y==1)[:,0]\n",
    "        ym=np.argwhere(y==-1)[:,0]\n",
    "        \n",
    "        np.random.shuffle(y0)\n",
    "        np.random.shuffle(y1)\n",
    "        \n",
    "        k0=int(y0.shape[0]*0.8)\n",
    "        k1=int(y1.shape[0]*0.8)\n",
    "        #pdb.set_trace()\n",
    "        y0_train=y0[:k0]\n",
    "        y1_train=y1[:k1]\n",
    "        \n",
    "        y_train=np.hstack([y0_train, y1_train])\n",
    "        y_train=np.sort(y_train)\n",
    "        train_mask[y_train]=True\n",
    "        test_mask[train_mask==False]=True\n",
    "        test_mask[ym]=False\n",
    "        \n",
    "        self.g.ndata['train_mask']=train_mask\n",
    "        self.g.ndata['test_mask']=test_mask\n",
    "    def apply_selfloop(self):\n",
    "        selfloop_src=list(self.g.nodes().detach().numpy())\n",
    "        selfloop_dst=list(self.g.nodes().detach().numpy())\n",
    "        self.g.add_edges(selfloop_src, selfloop_dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T06:18:49.906689Z",
     "start_time": "2019-07-01T06:18:49.901560Z"
    }
   },
   "outputs": [],
   "source": [
    "args={\n",
    "    'feature':feature,\n",
    "    'big_comm':big_comm,\n",
    "    'v_col':'nodeId',\n",
    "    'ft_col':numerical_ft,\n",
    "    'y_col':'shangbao',\n",
    "    'src_col':'src_nodeId',\n",
    "    'dst_col':'dst_nodeId',\n",
    "    'transformer':not_null_norm\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T06:18:50.713084Z",
     "start_time": "2019-07-01T06:18:50.644114Z"
    }
   },
   "outputs": [],
   "source": [
    "data=Dataset(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T06:18:51.223827Z",
     "start_time": "2019-07-01T06:18:51.217572Z"
    }
   },
   "outputs": [],
   "source": [
    "data.apply_mask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T06:18:51.752012Z",
     "start_time": "2019-07-01T06:18:51.739733Z"
    }
   },
   "outputs": [],
   "source": [
    "data.apply_selfloop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Structure Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T06:14:30.059621Z",
     "start_time": "2019-07-01T06:14:30.055214Z"
    }
   },
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T06:14:31.073040Z",
     "start_time": "2019-07-01T06:14:30.632355Z"
    }
   },
   "outputs": [],
   "source": [
    "figg=plt.figure(figsize=(14,7))\n",
    "ax1=figg.add_subplot(221)\n",
    "ax2=figg.add_subplot(222)\n",
    "ax3=figg.add_subplot(223)\n",
    "ax4=figg.add_subplot(224)\n",
    "ax1.plot(data.g.in_degrees().detach().numpy(),'.')\n",
    "ax2.hist(data.g.in_degrees().detach().numpy())\n",
    "ax3.plot(data.g.out_degrees().detach().numpy(),'.')\n",
    "ax4.hist(data.g.out_degrees().detach().numpy())\n",
    "figg.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T06:14:31.935699Z",
     "start_time": "2019-07-01T06:14:31.858139Z"
    }
   },
   "outputs": [],
   "source": [
    "nx_g=data.g.to_networkx()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GCN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T06:14:32.827526Z",
     "start_time": "2019-07-01T06:14:32.823227Z"
    }
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T06:18:57.816568Z",
     "start_time": "2019-07-01T06:18:57.809776Z"
    }
   },
   "outputs": [],
   "source": [
    "class Graph_Model(nn.Module):\n",
    "    def __init__(self, g, n_ftr, n_hidden, norm=True):\n",
    "        super(Graph_Model, self).__init__()\n",
    "        self.graph = g\n",
    "        self.gcn0 = GraphConv(n_ftr, n_hidden, activation= F.relu, norm=norm)\n",
    "        self.linear0 = nn.Linear(n_ftr + n_hidden, n_hidden)\n",
    "        nn.init.xavier_uniform(self.linear0.weight)\n",
    "        self.bn0=nn.BatchNorm1d(n_ftr + n_hidden)\n",
    "        self.bn1 = nn.BatchNorm1d(n_ftr + 2* n_hidden)\n",
    "        self.dp0 = nn.Dropout(0.3)\n",
    "        \n",
    "        self.linear1 = nn.Linear(n_ftr + 2* n_hidden, 1)\n",
    "        nn.init.xavier_uniform(self.linear1.weight)\n",
    "    def forward(self, input_):\n",
    "        h1=self.gcn0(input_, self.graph)  ## add graph convolution\n",
    "#         h1=h1+input_ ## equivalently with adding self-loop edges\n",
    "        h2=torch.cat((input_, h1), dim=1) ## add residual net\n",
    "        h5 = pipe(h2,\n",
    "                  self.bn0,\n",
    "                  self.linear0,\n",
    "                  F.relu\n",
    "                  )\n",
    "        h6 = torch.cat((h5, h2), dim=1) ## add the second residual net\n",
    "        h8 = pipe(h6,\n",
    "                  self.bn1,\n",
    "                  self.dp0,\n",
    "                  self.linear1,\n",
    "                  torch.sigmoid\n",
    "                 )\n",
    "        return h8\n",
    "def evaluate(model, feature, labels, test_mask):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(feature)\n",
    "        y_pred = logits[test_mask]\n",
    "        y_test = labels[test_mask]\n",
    "        ##y_pred_label = torch.\n",
    "    try:\n",
    "        mv =ks_2samp((y_pred[y_test<0.5]).detach().numpy(),(y_pred[y_test>0.5]).detach().numpy())\n",
    "    except:\n",
    "        mv = 'The prob pred by GCN remains the same for all verticals'\n",
    "    #pdb.set_trace()\n",
    "    return mv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T06:18:59.275887Z",
     "start_time": "2019-07-01T06:18:59.272355Z"
    }
   },
   "outputs": [],
   "source": [
    "args_={\n",
    "    'epochs': 200,\n",
    "    'lr':1e-3,\n",
    "    'num_hidden': 50\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T06:19:00.459549Z",
     "start_time": "2019-07-01T06:19:00.456043Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T06:20:16.748873Z",
     "start_time": "2019-07-01T06:20:16.736565Z"
    }
   },
   "outputs": [],
   "source": [
    "def main(args_):\n",
    "    features=torch.Tensor(data.g.ndata['feature'])\n",
    "    train_mask = torch.ByteTensor(data.g.ndata['train_mask'].astype(np.int32))\n",
    "    test_mask = torch.ByteTensor(data.g.ndata['test_mask'].astype(np.int32))\n",
    "    labels = torch.LongTensor(data.g.ndata['label'])\n",
    "    \n",
    "    model = Graph_Model(data.g, features.shape[1], args_['num_hidden'])\n",
    "    \n",
    "    # loss_fcn = F.nll_loss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = args_['lr'], weight_decay = 0.1)\n",
    "    dur = []\n",
    "    print(model)\n",
    "    #for param in model.parameters():\n",
    "    #    print(param, param.shape)\n",
    "    for epoch in range(args_['epochs']):\n",
    "        if epoch >= 3:\n",
    "            t0 = time.time()\n",
    "        model.train()\n",
    "        logits = model(features)\n",
    "        loss = F.nll_loss(logits[train_mask], labels[train_mask])\n",
    "        \n",
    "        if(epoch >= 3):\n",
    "            dur.append(time.time()- t0)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.zero_grad()\n",
    "        ks_value = evaluate(model, features, labels, test_mask)\n",
    "        \n",
    "        print(\"Epoch {:05d} | Time(s) {:4f} | Loss {:.4f} | KS_Value {} \\n\".format(epoch, np.mean(dur), loss.item(), ks_value))\n",
    "        with open(f'gcn_model.epoch{epoch}','wb') as f:\n",
    "            torch.save(model.state_dict(), f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T06:20:22.481125Z",
     "start_time": "2019-07-01T06:20:22.384495Z"
    }
   },
   "outputs": [],
   "source": [
    "main(args_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
