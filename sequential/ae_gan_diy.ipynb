{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-28T02:21:27.656972Z",
     "start_time": "2019-08-28T02:21:26.323268Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler\n",
    "from sklearn import metrics\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-28T02:21:32.845291Z",
     "start_time": "2019-08-28T02:21:30.437254Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = pd.read_pickle(\"x_train.pkl\") # iid feature with target(label) for train dataset\n",
    "X_test = pd.read_pickle(\"x_test.pkl\") # iid feature with target(label) for test dataset\n",
    "X_train_ = X_train.fillna(0)\n",
    "X_test_ = X_test.fillna(0)\n",
    "\n",
    "x_benign = X_train_[X_train_['label']==0]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "x_benign = scaler.fit_transform(x_benign.iloc[:,:-3])\n",
    "x_train = x_benign\n",
    "\n",
    "x_test = scaler.transform(X_test_.iloc[:,:-3])\n",
    "y_test = X_test_.iloc[:,-1]\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T08:38:29.578331Z",
     "start_time": "2019-08-23T08:38:29.574356Z"
    }
   },
   "source": [
    "#### Define Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T08:47:40.807753Z",
     "start_time": "2019-08-23T08:47:40.667563Z"
    }
   },
   "outputs": [],
   "source": [
    "# params\n",
    "input_dim = 372 # this is the number of features we extracted. Should be adjusted according to your feature set\n",
    "n_l1 = 1000\n",
    "n_l2 = 1000\n",
    "noise_dim = 50\n",
    "z_dim = 500\n",
    "batch_size = 100\n",
    "n_epochs = 100\n",
    "learning_rate = 0.001\n",
    "beta1 = 0.9\n",
    "results_path = './Results/'\n",
    "\n",
    "# Placeholders for input data and the targets\n",
    "x_input = tf.placeholder(dtype=tf.float32, shape=[None, input_dim], name='Input')\n",
    "x_target = tf.placeholder(dtype=tf.float32, shape=[batch_size, input_dim], name='Target')\n",
    "noise_distribution = tf.placeholder(dtype=tf.float32, shape=[batch_size, noise_dim], name='Noise_distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T08:38:18.691311Z",
     "start_time": "2019-08-23T08:38:18.687153Z"
    }
   },
   "source": [
    "#### Define Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T08:47:41.018109Z",
     "start_time": "2019-08-23T08:47:40.809452Z"
    }
   },
   "outputs": [],
   "source": [
    "def form_results():\n",
    "    \"\"\"\n",
    "    Forms folders for each run to store the tensorboard files, saved models and the log files.\n",
    "    :return: three string pointing to tensorboard, saved models and log paths respectively.\n",
    "    \"\"\"\n",
    "    folder_name = \"/{0}_{1}_{2}_{3}_{4}_{5}_Adversarial_Autoencoder\". \\\n",
    "        format(round(time.time()), z_dim, learning_rate, batch_size, n_epochs, beta1)\n",
    "    tensorboard_path = results_path + folder_name + '/Tensorboard'\n",
    "    saved_model_path = results_path + folder_name + '/Saved_models/'\n",
    "    log_path = results_path + folder_name + '/log'\n",
    "    if not os.path.exists(results_path + folder_name):\n",
    "        os.mkdir(results_path + folder_name)\n",
    "        os.mkdir(tensorboard_path)\n",
    "        os.mkdir(saved_model_path)\n",
    "        os.mkdir(log_path)\n",
    "    return tensorboard_path, saved_model_path, log_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T08:47:41.147816Z",
     "start_time": "2019-08-23T08:47:41.021913Z"
    }
   },
   "outputs": [],
   "source": [
    "def dense(x, n1, n2, name):\n",
    "    \"\"\"\n",
    "    Used to create a dense layer.\n",
    "    :param x: input tensor to the dense layer\n",
    "    :param n1: no. of input neurons\n",
    "    :param n2: no. of output neurons\n",
    "    :param name: name of the entire dense layer.i.e, variable scope name.\n",
    "    :return: tensor with shape [batch_size, n2]\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(name, reuse=None):\n",
    "        weights = tf.get_variable(\"weights\", shape=[n1, n2],\n",
    "                                  initializer=tf.random_normal_initializer(mean=0., stddev=0.01))\n",
    "        bias = tf.get_variable(\"bias\", shape=[n2], initializer=tf.constant_initializer(0.0))\n",
    "        out = tf.add(tf.matmul(x, weights), bias, name='matmul')\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T08:47:41.398354Z",
     "start_time": "2019-08-23T08:47:41.150942Z"
    }
   },
   "outputs": [],
   "source": [
    "# The autoencoder network\n",
    "def encoder(x, reuse=False):\n",
    "    \"\"\"\n",
    "    Encode part of the autoencoder.\n",
    "    :param x: input to the autoencoder\n",
    "    :param reuse: True -> Reuse the encoder variables, False -> Create or search of variables before creating\n",
    "    :return: tensor which is the hidden latent variable of the autoencoder.\n",
    "    \"\"\"\n",
    "    if reuse:\n",
    "        tf.get_variable_scope().reuse_variables()\n",
    "    with tf.name_scope('Encoder'):\n",
    "        e_dense_1 = tf.nn.relu(dense(x, input_dim, n_l1, 'e_dense_1'))\n",
    "        e_dense_2 = tf.nn.relu(dense(e_dense_1, n_l1, n_l2, 'e_dense_2'))\n",
    "        latent_variable = dense(e_dense_2, n_l2, z_dim, 'e_latent_variable')\n",
    "        return latent_variable\n",
    "\n",
    "\n",
    "def decoder(x, reuse=False):\n",
    "    \"\"\"\n",
    "    Decoder part of the autoencoder.\n",
    "    :param x: input to the decoder\n",
    "    :param reuse: True -> Reuse the decoder variables, False -> Create or search of variables before creating\n",
    "    :return: tensor which should ideally be the input given to the encoder.\n",
    "    \"\"\"\n",
    "    if reuse:\n",
    "        tf.get_variable_scope().reuse_variables()\n",
    "    with tf.name_scope('Decoder'):\n",
    "        d_dense_1 = tf.nn.relu(dense(x, z_dim, n_l2, 'd_dense_1'))\n",
    "        d_dense_2 = tf.nn.relu(dense(d_dense_1, n_l2, n_l1, 'd_dense_2'))\n",
    "        output = tf.nn.sigmoid(dense(d_dense_2, n_l1, input_dim, 'd_output'))\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T08:47:41.575709Z",
     "start_time": "2019-08-23T08:47:41.402702Z"
    }
   },
   "outputs": [],
   "source": [
    "# The GAN netowrk\n",
    "def generator(x, reuse=False):\n",
    "    if reuse:\n",
    "        tf.get_variable_scope().reuse_variables()\n",
    "    with tf.name_scope('Generator'):\n",
    "        dc_den1 = tf.nn.relu(dense(x, noise_dim, n_l1, name='ge_den1'))\n",
    "        dc_den2 = tf.nn.relu(dense(dc_den1, n_l1, n_l2, name='ge_den2'))\n",
    "        output = dense(dc_den2, n_l2, z_dim, name='ge_output')\n",
    "        return output\n",
    "\n",
    "\n",
    "def discriminator(x, reuse=False):\n",
    "    \"\"\"\n",
    "    Discriminator that is used to match the posterior distribution with a given prior distribution.\n",
    "    :param x: tensor of shape [batch_size, z_dim]\n",
    "    :param reuse: True -> Reuse the discriminator variables,\n",
    "                  False -> Create or search of variables before creating\n",
    "    :return: tensor of shape [batch_size, 1]\n",
    "    \"\"\"\n",
    "    if reuse:\n",
    "        tf.get_variable_scope().reuse_variables()\n",
    "    with tf.name_scope('Discriminator'):\n",
    "        dc_den1 = tf.nn.relu(dense(x, z_dim, n_l1, name='dc_den1'))\n",
    "        dc_den2 = tf.nn.relu(dense(dc_den1, n_l1, n_l2, name='dc_den2'))\n",
    "        output = dense(dc_den2, n_l2, 1, name='dc_output')\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T08:47:41.763856Z",
     "start_time": "2019-08-23T08:47:41.579143Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(x_train, x_test, y_test, train_model=True):\n",
    "    \"\"\"\n",
    "    Used to train the autoencoder by passing in the necessary inputs.\n",
    "    :param train_model: True -> Train the model, False -> Load the latest trained model and show the image grid.\n",
    "    :return: does not return anything\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(tf.get_variable_scope()):\n",
    "        encoder_output = encoder(x_input)\n",
    "        decoder_output = decoder(encoder_output)\n",
    "\n",
    "    with tf.variable_scope(tf.get_variable_scope()):\n",
    "        fake_sample = generator(noise_distribution)\n",
    "        d_fake = discriminator(fake_sample)\n",
    "        d_real = discriminator(encoder_output, reuse=True)\n",
    "\n",
    "    # Autoencoder loss\n",
    "    autoencoder_loss = tf.reduce_mean(tf.square(x_target - decoder_output))\n",
    " \n",
    "    # Discrimminator Loss\n",
    "    dc_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(d_fake), logits=d_fake))\n",
    "    dc_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.zeros_like(d_real), logits=d_real))\n",
    "    dc_loss = dc_loss_fake + dc_loss_real\n",
    "\n",
    "    # Generator loss\n",
    "    generator_loss = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(d_fake), logits=d_fake))\n",
    "\n",
    "    all_variables = tf.trainable_variables()\n",
    "    \n",
    "    dc_var = [var for var in all_variables if 'dc_' in var.name]\n",
    "    ge_var = [var for var in all_variables if 'ge_' in var.name] \n",
    "   \n",
    "\n",
    "    # Optimizers\n",
    "    autoencoder_optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate,\n",
    "                                                   beta1=beta1).minimize(autoencoder_loss)\n",
    "    discriminator_optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate,\n",
    "                                                     beta1=beta1).minimize(dc_loss, var_list=dc_var)\n",
    "    generator_optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate,\n",
    "                                                 beta1=beta1).minimize(generator_loss, var_list=ge_var)\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Tensorboard visualization\n",
    "    tf.summary.scalar(name='Autoencoder Loss', tensor=autoencoder_loss)\n",
    "    tf.summary.scalar(name='Discriminator Loss', tensor=dc_loss)\n",
    "    tf.summary.scalar(name='Generator Loss', tensor=generator_loss)\n",
    "    tf.summary.histogram(name='Encoder Distribution', values=encoder_output)\n",
    "    tf.summary.histogram(name='Fake Sample Distribution', values=fake_sample)\n",
    "    summary_op = tf.summary.merge_all()\n",
    "\n",
    "    # Saving the model\n",
    "    #saver = tf.train.Saver()\n",
    "    step = 0\n",
    "    with tf.Session() as sess:\n",
    "        if train_model:\n",
    "            tensorboard_path, saved_model_path, log_path = form_results()\n",
    "            sess.run(init)\n",
    "            writer = tf.summary.FileWriter(logdir=tensorboard_path, graph=sess.graph)\n",
    "            for i in range(n_epochs):\n",
    "                n_batches = int(x_train.shape[0] / batch_size)\n",
    "                print(\"------------------Epoch {}/{}------------------\".format(i, n_epochs))\n",
    "                for b in range(1, n_batches + 1):\n",
    "                    \n",
    "                    noise_dist = np.random.randn(batch_size, noise_dim) * 5.\n",
    "                    start, end = (b-1)*batch_size, b*batch_size\n",
    "                    batch_x = x_train[start:end]\n",
    "                    \n",
    "                    sess.run(autoencoder_optimizer, feed_dict={x_input: batch_x, x_target: batch_x})\n",
    "                    sess.run(discriminator_optimizer,\n",
    "                             feed_dict={x_input: batch_x, x_target: batch_x, noise_distribution: noise_dist})\n",
    "                    \n",
    "                    sess.run(generator_optimizer, feed_dict={noise_distribution: noise_dist})\n",
    "                    if b % 50 == 0:\n",
    "                        a_loss, d_loss, g_loss, summary = sess.run(\n",
    "                            [autoencoder_loss, dc_loss, generator_loss, summary_op],\n",
    "                            feed_dict={x_input: batch_x, x_target: batch_x,\n",
    "                                       noise_distribution: noise_dist})\n",
    "                        writer.add_summary(summary, global_step=step)\n",
    "                        print(\"Epoch: {}, iteration: {}\".format(i, b))\n",
    "                        print(\"Autoencoder Loss: {}\".format(a_loss))\n",
    "                        print(\"Discriminator Loss: {}\".format(d_loss))\n",
    "                        print(\"Generator Loss: {}\".format(g_loss))\n",
    "                        with open(log_path + '/log.txt', 'a') as log:\n",
    "                            log.write(\"Epoch: {}, iteration: {}\\n\".format(i, b))\n",
    "                            log.write(\"Autoencoder Loss: {}\\n\".format(a_loss))\n",
    "                            log.write(\"Discriminator Loss: {}\\n\".format(d_loss))\n",
    "                            log.write(\"Generator Loss: {}\\n\".format(g_loss))\n",
    "                    step += 1\n",
    "\n",
    "                #saver.save(sess, save_path=saved_model_path, global_step=step)\n",
    "        else:\n",
    "            # Get the latest results folder\n",
    "            all_results = os.listdir(results_path)\n",
    "            all_results.sort()\n",
    "            saver.restore(sess, save_path=tf.train.latest_checkpoint(results_path + '/' + all_results[-1] + '/Saved_models/'))\n",
    "            generate_image_grid(sess, op=decoder_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T08:47:53.360521Z",
     "start_time": "2019-08-23T08:47:41.767082Z"
    }
   },
   "outputs": [],
   "source": [
    "train(x_train, x_test, y_test, train_model=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-02T05:13:45.208289Z",
     "start_time": "2019-09-02T05:13:45.120054Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def plot_result(log_path):\n",
    "    #log_path = './Results/1566813215_1000_0.001_100_20_0.9_Adversarial_Autoencoder/log'\n",
    "    with open(log_path + '/log.txt', 'r') as log:\n",
    "        lines = log.readlines()\n",
    "        index, ae_loss, d_loss, g_loss = [], [], [], []\n",
    "        for line in lines:\n",
    "            if line.startswith('E'):\n",
    "                num = re.findall(r\"\\d+\\.?\\d*\",line)\n",
    "                index.append(f\"{num[0]}-{num[1]}\")\n",
    "            if line.startswith('Aut'):\n",
    "                val = re.findall(r\"\\d+\\.?\\d*\",line)\n",
    "                ae_loss.append(float(val[0]))\n",
    "            if line.startswith('D'):\n",
    "                val = re.findall(r\"\\d+\\.?\\d*\",line)\n",
    "                d_loss.append(float(val[0]))\n",
    "            if line.startswith('G'):\n",
    "                val = re.findall(r\"\\d+\\.?\\d*\",line)\n",
    "                g_loss.append(float(val[0]))  \n",
    "        return index, ae_loss, d_loss, g_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T02:14:10.339400Z",
     "start_time": "2019-08-27T02:14:10.323472Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "index1, ae_loss1, d_loss1, g_loss1 = plot_result('./Results/1566814923_1000_0.001_100_10_0.9_Adversarial_Autoencoder/log')\n",
    "\n",
    "index2, ae_loss2, d_loss2, g_loss2 = plot_result('./Results/1566814590_1000_0.001_100_10_0.9_Adversarial_Autoencoder/log')\n",
    "\n",
    "index3, ae_loss3, d_loss3, g_loss3 = plot_result('./Results/1566814170_1000_0.001_100_10_0.9_Adversarial_Autoencoder/log'\n",
    "index4, ae_loss4, d_loss4, g_loss4 = plot_result('./Results/1566813900_1000_0.001_100_10_0.9_Adversarial_Autoencoder/log')     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-02T05:20:52.457157Z",
     "start_time": "2019-09-02T05:20:52.449673Z"
    }
   },
   "outputs": [],
   "source": [
    "index, ae_loss, d_loss, g_loss, ks = plot_result('./Results/1566814170_1000_0.001_100_10_0.9_Adversarial_Autoencoder/log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-02T05:20:52.871877Z",
     "start_time": "2019-09-02T05:20:52.840717Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame([index, ae_loss, d_loss, g_loss, ks]).T\n",
    "df.columns = ['index', 'ae_loss', 'd_loss', 'g_loss', 'ks']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 比较不同修改对模型的影响"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 比较autoencoder loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T02:24:14.977055Z",
     "start_time": "2019-08-27T02:24:14.947153Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_ae = pd.DataFrame([ae_loss1, ae_loss2, ae_loss3, ae_loss4]).T\n",
    "df_ae.columns = ['iterate_encoder_and_ae_every_batch', 'iterate_encoder_and_ae_every_500_batches', \n",
    "                 'iterate_ae_not_encoder_every_500_batches',\n",
    "                 'not_iterate_encoder']\n",
    "df_ae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 比较d_loss, g_loss, ks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T02:16:29.534237Z",
     "start_time": "2019-08-27T02:16:29.490121Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame([d_loss1, g_loss1, ks1, d_loss2, g_loss2, ks2, d_loss3,\\\n",
    "                     g_loss3, ks3, d_loss4, g_loss4, ks4]).T\n",
    "df.columns = ['A_d_loss', 'A_g_loss', 'A_ks', 'B_d_loss', 'B_g_loss', 'B_ks', 'C_d_loss', 'C_g_loss', \\\n",
    "             'C_ks', 'D_d_loss', 'D_g_loss', 'D_ks']\n",
    "print(\"A: \")\n",
    "print(\"B: \")\n",
    "print(\"C: \")\n",
    "print(\"D: \")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 画一画d_loss, g_loss, ks的关系"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T02:42:07.692091Z",
     "start_time": "2019-08-27T02:42:07.677122Z"
    }
   },
   "outputs": [],
   "source": [
    "df_ = df[['A_d_loss', 'A_g_loss', 'A_ks']]\n",
    "df_ = df_[df_['A_d_loss']<10]\n",
    "df_ = df_[df_['A_g_loss']<10]\n",
    "df_good = df_[df_['A_ks']>0.1]\n",
    "df_bad = df_[df_['A_ks']<0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T02:44:35.262475Z",
     "start_time": "2019-08-27T02:44:35.254136Z"
    }
   },
   "outputs": [],
   "source": [
    "df_['square'] = df_['A_d_loss']**2 + df_['A_g_loss']**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d_loss, g_loss does not reflect the performance of gan\n",
    "\n",
    "decrease of loss does not bring ks improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T02:45:23.385086Z",
     "start_time": "2019-08-27T02:45:23.358645Z"
    }
   },
   "outputs": [],
   "source": [
    "df_.sort_values('square')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T02:42:09.179630Z",
     "start_time": "2019-08-27T02:42:08.994864Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure(figsize=(15,15))\n",
    "ax = plt.subplot(111, projection='3d') \n",
    "\n",
    "ax.scatter(df_good['A_d_loss'], df_good['A_g_loss'], df_good['A_ks'], c='b')  \n",
    "ax.scatter(df_bad['A_d_loss'], df_bad['A_g_loss'], df_bad['A_ks'], c='r')\n",
    "\n",
    "\n",
    "ax.set_zlabel('Z') \n",
    "ax.set_ylabel('Y')\n",
    "ax.set_xlabel('X')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
